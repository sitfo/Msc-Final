\documentclass[journal,10pt]{IEEEtran}
\usepackage{sharina}

\begin{document}
\input{title.tex}
\section{Project Description}
In recent years, Large Language Models (LLMs) have transformed the field of Natural Language Processing (NLP) with their impressive achievements. These models, which have been trained on extensive amounts of text data, have proven to be highly effective in solving various NLP tasks, like translating languages, answering questions, and generating text. Their ability to understand and create text that resembles human language has been truly remarkable and easy to appreciate.

However, despite their proficiency in many linguistic tasks, LLMs have faced challenges when it comes to Spatial Reasoning. Unlike tasks that focus solely on language, Spatial Reasoning involves comprehending and manipulating visual and spatial information. To shed light on this limitation, I plan to conduct an experiment using popular LLMs like GPT-4\cite{peng2023instruction}. The main goal will be to test how well they can handle Visual Spatial Reasoning, which is important for understanding the relationships between objects in visual content.

The experiment will center around a commonly used task called Visual Question Answering (VQA). This task requires machines to answer questions based on provided images. For example, when given an image showing different objects, the machine will be given inquiries such as "What is the spatial relationship between object A and object B?". By designing carefully constructed tests like these, we can assess the level of Spatial Reasoning ability demonstrated by LLMs in an easy-to-understand manner.

Through evaluating their performance in VQA and their capacity to comprehend and reason about spatial relationships in visual content, we can gain valuable insights into the Spatial Reasoning capabilities of LLMs. These findings will not only enhance our understanding of the strengths and limitations of these models but also pave the way for further advancements in NLP. This progress will bring us closer to developing more comprehensive and versatile language models that are accessible and easy to work with.

\section{Aims and Objectives}
In this section, we will provide a clear outline of the aims and objectives of this project. The aims represent the overarching goals that we aim to achieve, while the objectives outline the specific steps and milestones that will be undertaken to fulfill these goals. The aims and objectives of this project are outlined below:
\subsection{Aims}
\begin{enumerate}
    \item To investigate the Spatial Reasoning abilities of Large Language Models (LLMs), specifically focusing on their performance in Visual Question Answering (VQA) tasks.
    \item To understand the limitations and challenges faced by LLMs in comprehending and reasoning about spatial relationships in visual content.
    \item To assess the current state of Spatial Reasoning capabilities in leading LLMs, then compare with elder models and explore their improvement in this domain. Besides, to identify potential avenues for enhancing LLMs' Spatial Reasoning abilities.
\end{enumerate}
\subsection{Objectives}
\begin{enumerate}
    \item Design and develop a comprehensive experimental framework for evaluating LLMs' Spatial Reasoning abilities in VQA tasks.
    \item Curate a diverse dataset of visual stimuli and corresponding questions that require spatial reasoning skills to answer accurately.
    \item Conduct systematic experiments using the prepared dataset and modified LLMs to assess their performance and measure their level of Spatial Reasoning competence.
    \item Analyze the experimental results to identify patterns, trends, and challenges in LLMs' Spatial Reasoning capabilities.
    \item Provide insights into the strengths and limitations of current LLMs for Spatial Reasoning tasks, based on the experimental findings.
    \item Suggest potential avenues for enhancing LLMs' Spatial Reasoning abilities, such as incorporating multimodal information or novel architectural modifications.
    \item Contribute to the existing body of knowledge in the field of NLP by advancing our understanding of LLMs' Spatial Reasoning abilities and their implications for future research and development.
\end{enumerate}

In summary, this project aims to investigate the Spatial Reasoning abilities of LLMs, specifically in VQA tasks, by designing experiments, analyzing results, and providing insights for improvement. By achieving these objectives, the project will contribute to the advancement of NLP and enhance our understanding of LLMs' capabilities in comprehending and reasoning about spatial relationships in visual content.

\section{Output}
During the experimental phase, a rigorous evaluation was conducted on three preeminent LLM models: Visual ChatGPT\cite{wu2023visual}, Bard, and MiniGPT-4\cite{zhu2023minigpt4}.

Visual ChatGPT epitomizes a groundbreaking amalgamation of ChatGPT's linguistic prowess with Visual Foundation Models. This fusion facilitates a fluid interface with the AI, accommodating both textual and visual stimuli. The model excels in addressing complex visual inquiries, performing visual editing operations, and delivering polished outcomes. Its architectural design is meticulously crafted to integrate visual model data with ChatGPT, rendering it compatible with models that possess diverse I/O channels and those requiring visual feedback mechanisms. Empirical studies suggest that Visual ChatGPT presents an auspicious trajectory for amplifying ChatGPT's visual proficiencies in conjunction with Visual Foundation Models\cite{wu2023visual}.

Bard, a brainchild of Google, stands as a state-of-the-art conversational AI, presenting formidable competition to OpenAI's ChatGPT. Contemporary advancements have endowed Bard with the acumen to assimilate visual data concurrently with textual cues, signifying a paramount stride in multi-modal conversational AI. This enhancement enables Bard to discern and interpret visual entities, such as imagery, when steered by textual directives. While Bard's prowess in textual data management is undisputed, current scholarly endeavors are probing its aptitude in visual comprehension, illuminating its merits and potential areas of enhancement. Such scholarly pursuits are quintessential for identifying bottlenecks and guiding the evolution of multi-modal generative models\cite{qin2023good}.

MiniGPT-4, an avant-garde model, is architected to encapsulate the vast functionalities of the GPT-4 model in a more compact framework. It harmoniously integrates a static visual encoder with Vicuna, a colossal language model, via a distinctive projection layer. This setup empowers MiniGPT-4 to manifest a plethora of multi-modal generative proficiencies intrinsic to GPT-4, spanning from the generation of intricate image narratives to the creation of websites from handwritten blueprints. Remarkably, MiniGPT-4 has unveiled pioneering capabilities, such as crafting narratives influenced by visual stimuli and proffering solutions to visual conundrums. The model's training paradigm, emphasizing meticulously aligned and curated datasets, plays a pivotal role in bolstering its generative fidelity\cite{zhu2023minigpt4}.

The evaluative metrics employed are delineated in Table \ref{tab:question_template}. The assessment encompasses 80 question sets, dichotomized into two tiers: Easy and Hard. This stratification hinges on the quantity of objects within the image and their respective dimensions. Images containing no more than five objects, with at least one object dominating a substantial segment of the frame, fall under the 'Easy' bracket. In contrast, images that showcase in excess of five objects, where each entity maintains a relatively diminutive presence, are designated as 'Hard'. An exhaustive exposition of the question sets, coupled with their respective outputs, is cataloged in Appendices \ref{Easy} and \ref{Hard}.
\begin{table}[ht]
    \centering
    \caption{Question Template}
    \label{tab:question_template}
    \begin{tabular}{|p{0.4\linewidth}|p{0.5\linewidth}|}
        \hline
        \textbf{Input Image} &\textbf{Questions}\\
        \hline
        \begin{center} \includegraphics[width=\linewidth]{../image set/hard/000000104739.jpg} \end{center}
        & \begin{enumerate}
            \item Can you describe the spatial relationship between these two laptops?
            \item Is this black laptop on the left of the white laptop?
            \item Show me your specific reasoning steps that lead you to the answer, better in detailed explanation.
        \end{enumerate}\\
        \hline
        \end{tabular}
\end{table}

The evaluation framework is anchored around three critical dimensions:
\begin{enumerate}
\item Scrutinizing the model's proficiency in interpreting spatial relationships between object pairs.
\item Investigating the model's capability in addressing inquiries underscored by definitive guiding terms.
\item Gauging the model's expertise in articulating comprehensive justifications for its determinations.
\end{enumerate}
The intentional exclusion of accuracy metrics for the foremost inquiry stems from Bard's unconventional operational paradigm. Specifically, Bard fabricates an extensive spectrum of plausible determinations, engendering complexities in unequivocally validating the authenticity of a response. This modus operandi not only challenges the traditional paradigm of a 'correct' answer but also complicates the endeavor of forging balanced juxtapositions with counterpart models.

Consequently, the principal accuracy metric is extrapolated from the responses to the second dimension, as it provides a more tangible criterion for adjudicating veracity. The inaugural and concluding inquiries serve as pivotal touchstones in appraising the model's depth of comprehension and inferential reasoning prowess. The gold standard for accuracy is predicated on the congruence of the response with the spatial relationship explicated in the directive or the assertion of a plausible spatial interrelation between visualized entities.

The exactitude metrics associated with the triad of models are meticulously chronicled in Table \ref{tab:accuracy}. For a deeper analytical dissection, readers are referred to Tables \ref{tab:easy-acc} and \ref{tab:hard-acc}.
\begin{table}[h]
    \centering
    \caption{Accuracy of Three Models}
    \label{tab:accuracy}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \multicolumn{1}{c}{Model} & Easy      & Hard & Total \\ \midrule
    Bard                      &  $42.5\%$ & $45.0\%$ &  $43.75\%$ \\
    Visual ChatGPT            &  $55.0\%$ & $60.0\%$ &  $57.5\%$  \\
    MiniGPT-4                 &  $62.5\%$ & $52.5\%$ &  $57.5\%$  \\ \bottomrule
    \end{tabular}
\end{table}

Upon scrutinizing the accuracy metrics, it is manifest that the efficacy of the three models warrants further refinement. Within this cohort, Bard persistently lags in performance across the spectrum of question sets. Visual ChatGPT, while demonstrating certain susceptibilities in the 'Easy' segment, outpaces Bard and commendably thrives in the 'Hard' segment. In contrast, MiniGPT-4 distinguishes itself in the 'Easy' segment but grapples with sustaining its preeminence in the 'Hard' segment. A nuanced dissection of these observations will be expounded upon in the ensuing section.

\section{Evaluation}
In this section, evaluation will be conducted on the three models based on the experiment results. The evaluation will be divided into three parts: analysis of the accuracy, analysis of the reasoning process and discussion of the potential improvements.
\subsection{Analysis of the Accuracy}

\subsection{Analysis of the Reasoning Process}

\subsection{Discussion of the Potential Improvements}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\appendices
\input{Chart.tex}
\label{Chart}
\clearpage
\input{../Q&A/easy.tex}
\label{Easy}
\input{../Q&A/hard.tex}
\label{Hard}



\end{document}