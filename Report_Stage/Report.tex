\documentclass[journal,10pt]{IEEEtran}
\usepackage{sharina}

\begin{document}
\input{title.tex}
\section{Project Description}
In recent years, Large Language Models (LLMs) have transformed the field of Natural Language Processing (NLP) with their impressive achievements. These models, which have been trained on extensive amounts of text data, have proven to be highly effective in solving various NLP tasks, like translating languages, answering questions, and generating text. Their ability to understand and create text that resembles human language has been truly remarkable and easy to appreciate.

However, despite their proficiency in many linguistic tasks, LLMs have faced challenges when it comes to Spatial Reasoning. Unlike tasks that focus solely on language, Spatial Reasoning involves comprehending and manipulating visual and spatial information. To shed light on this limitation, I plan to conduct an experiment using popular LLMs like GPT-4\cite{peng2023instruction}. The main goal will be to test how well they can handle Visual Spatial Reasoning, which is important for understanding the relationships between objects in visual content.

The experiment will center around a commonly used task called Visual Question Answering (VQA). This task requires machines to answer questions based on provided images. For example, when given an image showing different objects, the machine will be given inquiries such as "What is the spatial relationship between object A and object B?". By designing carefully constructed tests like these, we can assess the level of Spatial Reasoning ability demonstrated by LLMs in an easy-to-understand manner.

Through evaluating their performance in VQA and their capacity to comprehend and reason about spatial relationships in visual content, we can gain valuable insights into the Spatial Reasoning capabilities of LLMs. These findings will not only enhance our understanding of the strengths and limitations of these models but also pave the way for further advancements in NLP. This progress will bring us closer to developing more comprehensive and versatile language models that are accessible and easy to work with.

\section{Aims and Objectives}
In this section, we will provide a clear outline of the aims and objectives of this project. The aims represent the overarching goals that we aim to achieve, while the objectives outline the specific steps and milestones that will be undertaken to fulfill these goals. The aims and objectives of this project are outlined below:
\subsection{Aims}
\begin{enumerate}
    \item To investigate the Spatial Reasoning abilities of Large Language Models (LLMs), specifically focusing on their performance in Visual Question Answering (VQA) tasks.
    \item To understand the limitations and challenges faced by LLMs in comprehending and reasoning about spatial relationships in visual content.
    \item To assess the current state of Spatial Reasoning capabilities in leading LLMs, then compare with elder models and explore their improvement in this domain. Besides, to identify potential avenues for enhancing LLMs' Spatial Reasoning abilities.
\end{enumerate}
\subsection{Objectives}
\begin{enumerate}
    \item Design and develop a comprehensive experimental framework for evaluating LLMs' Spatial Reasoning abilities in VQA tasks.
    \item Curate a diverse dataset of visual stimuli and corresponding questions that require spatial reasoning skills to answer accurately.
    \item Conduct systematic experiments using the prepared dataset and modified LLMs to assess their performance and measure their level of Spatial Reasoning competence.
    \item Analyze the experimental results to identify patterns, trends, and challenges in LLMs' Spatial Reasoning capabilities.
    \item Provide insights into the strengths and limitations of current LLMs for Spatial Reasoning tasks, based on the experimental findings.
    \item Suggest potential avenues for enhancing LLMs' Spatial Reasoning abilities, such as incorporating multimodal information or novel architectural modifications.
    \item Contribute to the existing body of knowledge in the field of NLP by advancing our understanding of LLMs' Spatial Reasoning abilities and their implications for future research and development.
\end{enumerate}

In summary, this project aims to investigate the Spatial Reasoning abilities of LLMs, specifically in VQA tasks, by designing experiments, analyzing results, and providing insights for improvement. By achieving these objectives, the project will contribute to the advancement of NLP and enhance our understanding of LLMs' capabilities in comprehending and reasoning about spatial relationships in visual content.

\section{Output}
During the experimental phase, an exhaustive assessment was undertaken on three leading LLM (Language and Logic Model) paradigms: Visual ChatGPT\cite{wu2023visual}, Bard, and MiniGPT-4\cite{zhu2023minigpt4}.

Visual ChatGPT represents a pioneering convergence of ChatGPT's linguistic capabilities with Visual Foundation Models. This synergy offers an enhanced interface with the AI, seamlessly integrating both textual and visual inputs. The model demonstrates proficiency in addressing intricate visual queries, executing visual editing tasks, and producing refined results. Its architectural framework is intricately designed to amalgamate visual model data with ChatGPT, making it adaptable to models with varied I/O channels and those necessitating visual feedback systems. Empirical research indicates that Visual ChatGPT offers a promising avenue for augmenting ChatGPT's visual capabilities in tandem with Visual Foundation Models\cite{wu2023visual}.

Bard, a prodigious creation from Google, emerges as a cutting-edge conversational AI, posing significant competition to OpenAI's ChatGPT. Recent innovations have equipped Bard with the capability to process visual data in tandem with textual information, marking a significant leap in multi-modal conversational AI. This advancement allows Bard to recognize and analyze visual components, such as images, when directed by textual prompts. While Bard's expertise in textual data processing is well-established, ongoing academic investigations are exploring its proficiency in visual understanding, highlighting its strengths and potential areas for refinement. Such academic endeavors are crucial for pinpointing limitations and steering the progression of multi-modal generative models\cite{qin2023good}.

MiniGPT-4, a trailblazing model, is engineered to encompass the expansive features of the GPT-4 model within a streamlined framework. It seamlessly fuses a static visual encoder with Vicuna, a robust language model, through a unique projection layer. This configuration enables MiniGPT-4 to exhibit a wide range of multi-modal generative capabilities inherent to GPT-4, ranging from crafting detailed image narratives to generating websites from handwritten schematics. Notably, MiniGPT-4 has revealed innovative competencies, such as formulating narratives influenced by visual cues and offering resolutions to visual challenges. The model's training methodology, focusing on precisely aligned and refined datasets, is instrumental in enhancing its generative accuracy\cite{zhu2023minigpt4}.

The metrics utilized for evaluation are detailed in Table \ref{tab:question_template}. The analysis comprises 80 question sets, bifurcated into two categories: Easy and Hard. This classification is predicated on the number of objects within the image and their respective scales. Images featuring no more than five objects, with a predominant object occupying a significant portion of the frame, are categorized as 'Easy'. Conversely, images displaying more than five objects, where each item has a comparatively modest presence, are labeled as 'Hard'. A comprehensive overview of the question sets, along with their corresponding outputs, is documented in Appendices \ref{Easy} and \ref{Hard}.
\begin{table}[ht]
    \centering
    \caption{Question Template}
    \label{tab:question_template}
    \begin{tabular}{|p{0.4\linewidth}|p{0.5\linewidth}|}
        \hline
        \textbf{Input Image} &\textbf{Questions}\\
        \hline
        \begin{center} \includegraphics[width=\linewidth]{../image set/hard/000000104739.jpg} \end{center}
        & \begin{enumerate}
            \item Can you describe the spatial relationship between these two laptops?
            \item Is this black laptop on the left of the white laptop?
            \item Show me your specific reasoning steps that lead you to the answer, better in detailed explanation.
        \end{enumerate}\\
        \hline
        \end{tabular}
\end{table}

The evaluation framework is structured around three pivotal dimensions:
\begin{enumerate}
\item Assessing the model's aptitude in discerning spatial relationships among object pairs.
\item Analyzing the model's competence in addressing queries emphasized by specific guiding terminologies.
\item Evaluating the model's skill in providing thorough rationales for its conclusions.
\end{enumerate}
The deliberate omission of accuracy metrics for the primary query is attributed to Bard's novel operational approach. In essence, Bard generates a broad array of potential conclusions, introducing challenges in conclusively affirming the veracity of a given response. This approach not only redefines the conventional notion of a 'correct' answer but also complicates the task of drawing equitable comparisons with analogous models.

As a result, the primary accuracy metric is derived from the responses to the second dimension, offering a more concrete benchmark for assessing truthfulness. The initial and final queries act as essential markers in gauging the model's depth of understanding and its capacity for inferential reasoning. The benchmark for accuracy is based on the alignment of the response with the spatial relationship delineated in the directive or the proposition of a credible spatial correlation among visual entities.

The precision metrics pertaining to the trio of models are diligently documented in Table \ref{tab:accuracy}. For a more granular analytical insight, readers are directed to Tables \ref{tab:easy-acc} and \ref{tab:hard-acc}.
\begin{table}[h]
    \centering
    \caption{Accuracy of Three Models}
    \label{tab:accuracy}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \multicolumn{1}{c}{Model} & Easy      & Hard & Total \\ \midrule
    Bard                      &  $45.0\%$ & $42.5\%$ &  $43.75\%$ \\
    Visual ChatGPT            &  $62.5\%$ & $57.5\%$ &  $60.0\%$  \\
    MiniGPT-4                 &  $65.0\%$ & $57.5\%$ &  $61.25\%$  \\ \bottomrule
    \end{tabular}
\end{table}

Based on the metrics, the MiniGPT-4 model manifests superior accuracy, succeeded by Visual ChatGPT and subsequently Bard. Notably, Visual ChatGPT's performance closely parallels that of MiniGPT-4. Furthermore, the accuracy across the two question sets remains relatively consistent for each model. However, it's evident that the accuracy for the 'Easy' question set surpasses that of the 'Hard' set for every model. A more granular analysis will be presented in the ensuing section.

\section{Evaluation}
In this section, we will conduct an in-depth evaluation of the three models, deriving insights from the empirical results. The assessment will be segmented into three salient dimensions:Examination of Accuracy, Scrutiny of the Reasoning Process, and Deliberative Discussions.
\subsection{Examination of Accuracy}
The detailed wrong answer analysis is documented by Table \ref{tab:error-easy} and \ref{tab:error-hard} in Appendix \ref{Chart}. The analysis is predicated on the following categories:
\begin{itemize}
    \item \textbf{Left-Right Confusion:} The model confuses the left and right sides of two objects.
    \item \textbf{Recognition Error:} The model fails to recognize the object in the image or get the wrong object.
    \item \textbf{Judgement Fault in Front and Back:} The model fails to judge the front and back of the object or the spatial relationship between the front and back of the object.
    \item \textbf{Positional Fault:} The model gives the wrong spatial relationship between two objects and it is quite different from the image.
    \item \textbf{Item Misorientation:} The model gives the wrong spatial relationship because of getting a wrong orientation of the object.
    \item \textbf{Confused Expression:} The model gives a conflicting answer.
    \item \textbf{No Exact Answer:} The model gives an answer that is not sure or not exact.
    \item \textbf{No Response:} The model gives no response.
    \item \textbf{Error Judgement in Post:} The model give the wrong answer about the post of people or animals in the image.
    \item \textbf{Logic Fault:} The model gives the wrong answer because of the wrong logic.
    \item \textbf{Vertical Fault:} The model gives the wrong answer in the vertical direction.
\end{itemize}

Firstly, in Easy part, Bard has the most type of errors, most of them are 'judgement fault in front and back', the most significant example is in Question 14 and Question 37

\subsection{Scrutiny of the Reasoning Process}

\subsection{Deliberative Discussions}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\appendices
\input{Chart.tex}
\label{Chart}
\clearpage
\input{../Q&A/easy.tex}
\label{Easy}
\input{../Q&A/hard.tex}
\label{Hard}



\end{document}