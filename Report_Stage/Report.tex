\documentclass[journal,10pt]{IEEEtran}
\usepackage{sharina}

\begin{document}
\input{title.tex}
\section{Project Description}
In recent years, Large Language Models (LLMs) have transformed the field of Natural Language Processing (NLP) with their impressive achievements. These models, which have been trained on extensive amounts of text data, have proven to be highly effective in solving various NLP tasks, like translating languages, answering questions, and generating text. Their ability to understand and create text that resembles human language has been truly remarkable and easy to appreciate.

However, despite their proficiency in many linguistic tasks, LLMs have faced challenges when it comes to Spatial Reasoning. Unlike tasks that focus solely on language, Spatial Reasoning involves comprehending and manipulating visual and spatial information. To shed light on this limitation, I plan to conduct an experiment using popular LLMs like GPT-4\cite{peng2023instruction}. The main goal will be to test how well they can handle Visual Spatial Reasoning, which is important for understanding the relationships between objects in visual content.

The experiment will center around a commonly used task called Visual Question Answering (VQA). This task requires machines to answer questions based on provided images. For example, when given an image showing different objects, the machine will be given inquiries such as "What is the spatial relationship between object A and object B?". By designing carefully constructed tests like these, we can assess the level of Spatial Reasoning ability demonstrated by LLMs in an easy-to-understand manner.

Through evaluating their performance in VQA and their capacity to comprehend and reason about spatial relationships in visual content, we can gain valuable insights into the Spatial Reasoning capabilities of LLMs. These findings will not only enhance our understanding of the strengths and limitations of these models but also pave the way for further advancements in NLP. This progress will bring us closer to developing more comprehensive and versatile language models that are accessible and easy to work with.

\section{Aims and Objectives}
In this section, we will provide a clear outline of the aims and objectives of this project. The aims represent the overarching goals that we aim to achieve, while the objectives outline the specific steps and milestones that will be undertaken to fulfill these goals. The aims and objectives of this project are outlined below:
\subsection{Aims}
\begin{enumerate}
    \item To investigate the Spatial Reasoning abilities of Large Language Models (LLMs), specifically focusing on their performance in Visual Question Answering (VQA) tasks.
    \item To understand the limitations and challenges faced by LLMs in comprehending and reasoning about spatial relationships in visual content.
    \item To assess the current state of Spatial Reasoning capabilities in leading LLMs, then compare with elder models and explore their improvement in this domain. Besides, to identify potential avenues for enhancing LLMs' Spatial Reasoning abilities.
\end{enumerate}
\subsection{Objectives}
\begin{enumerate}
    \item Design and develop a comprehensive experimental framework for evaluating LLMs' Spatial Reasoning abilities in VQA tasks.
    \item Curate a diverse dataset of visual stimuli and corresponding questions that require spatial reasoning skills to answer accurately.
    \item Conduct systematic experiments using the prepared dataset and modified LLMs to assess their performance and measure their level of Spatial Reasoning competence.
    \item Analyze the experimental results to identify patterns, trends, and challenges in LLMs' Spatial Reasoning capabilities.
    \item Provide insights into the strengths and limitations of current LLMs for Spatial Reasoning tasks, based on the experimental findings.
    \item Suggest potential avenues for enhancing LLMs' Spatial Reasoning abilities, such as incorporating multimodal information or novel architectural modifications.
    \item Contribute to the existing body of knowledge in the field of NLP by advancing our understanding of LLMs' Spatial Reasoning abilities and their implications for future research and development.
\end{enumerate}

In summary, this project aims to investigate the Spatial Reasoning abilities of LLMs, specifically in VQA tasks, by designing experiments, analyzing results, and providing insights for improvement. By achieving these objectives, the project will contribute to the advancement of NLP and enhance our understanding of LLMs' capabilities in comprehending and reasoning about spatial relationships in visual content.

\section{Output}
In the experiment stage, three leading LLM models were evaluated: Visual ChatGPT\cite{wu2023visual}, Bard, and MiniGPT-4\cite{zhu2023minigpt4}.

Visual ChatGPT represents a pioneering integration of ChatGPT's linguistic capabilities with Visual Foundation Models. This allows for a seamless interaction with the AI using both textual and visual inputs. The model is adept at handling intricate visual queries, executing visual editing tasks, and providing refined results. Its architecture is tailored to incorporate visual model data into ChatGPT, making it compatible with models that have multiple I/O channels and those necessitating visual feedback. Research indicates that Visual ChatGPT offers a promising avenue for enhancing ChatGPT's visual competencies through the synergy with Visual Foundation Models\cite{wu2023visual}.

Bard, developed by Google, is a cutting-edge conversational AI that poses significant competition to OpenAI's ChatGPT. Recent enhancements have equipped Bard with the capability to process visual data in tandem with textual prompts, marking a monumental leap in multi-modal conversational AI. This feature empowers Bard to comprehend and interpret visual content, such as images, when guided by textual prompts. While Bard excels in managing textual data, ongoing research has been investigating its proficiency in visual comprehension, shedding light on its strengths and areas for refinement. Such inquiries are pivotal for pinpointing challenges and steering the progression of multi-modal generative models\cite{qin2023good}.

MiniGPT-4 is a sophisticated model designed to encapsulate the functionalities of the expansive GPT-4 model in a streamlined format. It synergizes a static visual encoder with Vicuna, a large language model, through a unique projection layer. This configuration allows MiniGPT-4 to demonstrate many of the multi-modal generative capabilities inherent in GPT-4, ranging from generating intricate image descriptions to crafting websites from handwritten drafts. Notably, MiniGPT-4 has exhibited novel capabilities, such as devising narratives inspired by visuals and offering solutions to visual challenges. The model's training methodology, which prioritizes meticulously curated and aligned datasets, has been instrumental in augmenting its generative reliability\cite{zhu2023minigpt4}.

The evaluation metrics are detailed in Table \ref{tab:question_template}. The test comprises 80 question sets, bifurcated into two categories: Easy and Hard. The categorization is based on the number of objects in the image and their relative sizes. Images with no more than five objects, where at least one object occupies a significant portion of the frame, are classified as 'Easy'. Conversely, images featuring more than five objects, with each object being relatively smaller, are categorized as 'Hard'. Comprehensive details of the question sets and their corresponding outputs can be found in Appendices \ref{Easy} and \ref{Hard}.
\begin{table}[ht]
    \centering
    \caption{Question Template}
    \label{tab:question_template}
    \begin{tabular}{|p{0.4\linewidth}|p{0.5\linewidth}|}
        \hline
        \textbf{Input Image} &\textbf{Questions}\\
        \hline
        \begin{center} \includegraphics[width=\linewidth]{../image set/hard/000000104739.jpg} \end{center}
        & \begin{enumerate}
            \item Can you describe the spatial relationship between these two laptops?
            \item Is this black laptop on the left of the white laptop?
            \item Show me your specific reasoning steps that lead you to the answer, better in detailed explanation.
        \end{enumerate}\\
        \hline
        \end{tabular}
\end{table}

The evaluation encompasses three distinct questions:
\begin{enumerate}
    \item Assessing the model's proficiency in comprehending spatial relationships between two objects.
    \item Evaluating the model's capability to respond to queries with direct leading terms.
    \item Gauging the model's aptitude in elucidating its answers.
\end{enumerate}
The decision to exclude the accuracy calculation for the first question stems from Bard's unconventional approach. Specifically, Bard provides an exhaustive set of potential answers, complicating the determination of correctness. This method not only challenges the definition of a correct response but also skews comparisons with other models.

Consequently, accuracy is primarily derived from the responses to the second question, which offers a more tangible basis for discerning correctness. Both the first and third questions serve to appraise the model's comprehension and reasoning skills. The criterion for accuracy is whether the response adheres to the spatial relationship presented in the query or establishes a plausible spatial relationship between objects depicted in images.

The accuracy metrics for the three models are presented in Table \ref{tab:accuracy}. For a more granular view, refer to Tables \ref{tab:easy-acc} and \ref{tab:hard-acc}.
\begin{table}[h]
    \centering
    \caption{Accuracy of Three Models}
    \label{tab:accuracy}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \multicolumn{1}{c}{Model} & Easy      & Hard & Total \\ \midrule
    Bard                      &  $42.5\%$ & $45.0\%$ &  $43.75\%$ \\
    Visual ChatGPT            &  $55.0\%$ & $60.0\%$ &  $57.5\%$  \\
    MiniGPT-4                 &  $62.5\%$ & $52.5\%$ &  $57.5\%$  \\ \bottomrule
    \end{tabular}
\end{table}

Upon examining the accuracy chart, it becomes evident that the performance of the three models leaves room for improvement. Among them, Bard consistently underperforms across all question sets. While Visual ChatGPT exhibits some vulnerabilities in the 'Easy' category, it surpasses Bard and notably excels in the 'Hard' category. Conversely, MiniGPT-4 stands out in the 'Easy' set but struggles to maintain its lead in the 'Hard' set. A comprehensive evaluation of these findings will be elaborated upon in the subsequent section.

\section{Evaluation}
In this section, evaluation 

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\appendices
\input{Chart.tex}
\label{Chart}
\clearpage
\input{../Q&A/easy.tex}
\label{Easy}
\input{../Q&A/hard.tex}
\label{Hard}



\end{document}