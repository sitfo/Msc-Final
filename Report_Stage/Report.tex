\documentclass[journal,10pt]{IEEEtran}
\usepackage{sharina}

\begin{document}
\input{title.tex}
\section{Project Description}
In recent years, Large Language Models (LLMs) have transformed the field of Natural Language Processing (NLP) with their impressive achievements. These models, which have been trained on extensive amounts of text data, have proven to be highly effective in solving various NLP tasks, like translating languages, answering questions, and generating text. Their ability to understand and create text that resembles human language has been truly remarkable and easy to appreciate.

However, despite their proficiency in many linguistic tasks, LLMs have faced challenges when it comes to Spatial Reasoning. Unlike tasks that focus solely on language, Spatial Reasoning involves comprehending and manipulating visual and spatial information. To shed light on this limitation, I plan to conduct an experiment using popular LLMs like GPT-4\cite{peng2023instruction}. The main goal will be to test how well they can handle Visual Spatial Reasoning, which is important for understanding the relationships between objects in visual content.

The experiment will center around a commonly used task called Visual Question Answering (VQA). This task requires machines to answer questions based on provided images. For example, when given an image showing different objects, the machine will be given inquiries such as "What is the spatial relationship between object A and object B?". By designing carefully constructed tests like these, we can assess the level of Spatial Reasoning ability demonstrated by LLMs in an easy-to-understand manner.

Through evaluating their performance in VQA and their capacity to comprehend and reason about spatial relationships in visual content, we can gain valuable insights into the Spatial Reasoning capabilities of LLMs. These findings will not only enhance our understanding of the strengths and limitations of these models but also pave the way for further advancements in NLP. This progress will bring us closer to developing more comprehensive and versatile language models that are accessible and easy to work with.

\section{Aims and Objectives}
In this section, we will provide a clear outline of the aims and objectives of this project. The aims represent the overarching goals that we aim to achieve, while the objectives outline the specific steps and milestones that will be undertaken to fulfill these goals. The aims and objectives of this project are outlined below:
\subsection{Aims}
\begin{enumerate}
    \item To investigate the Spatial Reasoning abilities of Large Language Models (LLMs), specifically focusing on their performance in Visual Question Answering (VQA) tasks.
    \item To understand the limitations and challenges faced by LLMs in comprehending and reasoning about spatial relationships in visual content.
    \item To assess the current state of Spatial Reasoning capabilities in leading LLMs, then compare with elder models and explore their improvement in this domain. Besides, to identify potential avenues for enhancing LLMs' Spatial Reasoning abilities.
\end{enumerate}
\subsection{Objectives}
\begin{enumerate}
    \item Design and develop a comprehensive experimental framework for evaluating LLMs' Spatial Reasoning abilities in VQA tasks.
    \item Curate a diverse dataset of visual stimuli and corresponding questions that require spatial reasoning skills to answer accurately.
    \item Conduct systematic experiments using the prepared dataset and modified LLMs to assess their performance and measure their level of Spatial Reasoning competence.
    \item Analyze the experimental results to identify patterns, trends, and challenges in LLMs' Spatial Reasoning capabilities.
    \item Provide insights into the strengths and limitations of current LLMs for Spatial Reasoning tasks, based on the experimental findings.
    \item Suggest potential avenues for enhancing LLMs' Spatial Reasoning abilities, such as incorporating multimodal information or novel architectural modifications.
    \item Contribute to the existing body of knowledge in the field of NLP by advancing our understanding of LLMs' Spatial Reasoning abilities and their implications for future research and development.
\end{enumerate}

In summary, this project aims to investigate the Spatial Reasoning abilities of LLMs, specifically in VQA tasks, by designing experiments, analyzing results, and providing insights for improvement. By achieving these objectives, the project will contribute to the advancement of NLP and enhance our understanding of LLMs' capabilities in comprehending and reasoning about spatial relationships in visual content.

\section{Output}
During the experimental phase, an exhaustive assessment was undertaken on three leading LLM (Language and Logic Model) paradigms: Visual ChatGPT\cite{wu2023visual}, Bard, and MiniGPT-4\cite{zhu2023minigpt4}.

Visual ChatGPT, a fusion of ChatGPT's linguistic strengths with Visual Foundation Models, offers a sophisticated AI interface that adeptly integrates textual and visual modalities. This model excels in handling complex visual tasks, with its architecture tailored to merge visual data seamlessly with ChatGPT, ensuring adaptability across diverse I/O configurations\cite{wu2023visual}.

Bard, Google's avant-garde conversational AI, stands as a formidable contender to OpenAI's ChatGPT. Enhanced with the ability to co-process visual and textual data, Bard signifies a leap in multi-modal conversational AI. Its prowess in textual analytics is undisputed, but its visual comprehension is under rigorous academic scrutiny, aiming to refine its multi-modal capabilities\cite{qin2023good}.

MiniGPT-4 encapsulates GPT-4's vast attributes within a compact design, merging a visual encoder with Vicuna, a potent language model. This design equips MiniGPT-4 with diverse multi-modal generative skills, from intricate narrative generation to website creation from sketches. Emphasizing data alignment in its training, MiniGPT-4 ensures high generative precision\cite{zhu2023minigpt4}.

The metrics utilized for evaluation are detailed in Table \ref{tab:question_template}. The analysis comprises 80 question sets, bifurcated into two categories: Easy and Hard. This classification is predicated on the number of objects within the image and their respective scales. Images featuring no more than five objects, with a predominant object occupying a significant portion of the frame, are categorized as 'Easy'. Conversely, images displaying more than five objects, where each item has a comparatively modest presence, are labeled as 'Hard'. A comprehensive overview of the question sets, along with their corresponding outputs, is documented in Appendices \ref{Easy} and \ref{Hard}.
\begin{table}[ht]
    \centering
    \caption{Question Template}
    \label{tab:question_template}
    \begin{tabular}{|p{0.4\linewidth}|p{0.5\linewidth}|}
        \hline
        \textbf{Input Image} &\textbf{Questions}\\
        \hline
        \begin{center} \includegraphics[width=\linewidth]{../image set/hard/000000104739.jpg} \end{center}
        & \begin{enumerate}
            \item Can you describe the spatial relationship between these two laptops?
            \item Is this black laptop on the left of the white laptop?
            \item Show me your specific reasoning steps that lead you to the answer, better in detailed explanation.
        \end{enumerate}\\
        \hline
        \end{tabular}
\end{table}

The evaluation framework is structured around three pivotal dimensions:
\begin{enumerate}
\item Assessing the model's aptitude in discerning spatial relationships among object pairs.
\item Analyzing the model's competence in addressing queries emphasized by specific guiding terminologies.
\item Evaluating the model's skill in providing thorough rationales for its conclusions.
\end{enumerate}
The deliberate omission of accuracy metrics for the primary query is attributed to Bard's novel operational approach. In essence, Bard generates a broad array of potential conclusions, introducing challenges in conclusively affirming the veracity of a given response. This approach not only redefines the conventional notion of a 'correct' answer but also complicates the task of drawing equitable comparisons with analogous models.

As a result, the primary accuracy metric is derived from the responses to the second dimension, offering a more concrete benchmark for assessing truthfulness. The initial and final queries act as essential markers in gauging the model's depth of understanding and its capacity for inferential reasoning. The benchmark for accuracy is based on the alignment of the response with the spatial relationship delineated in the directive or the proposition of a credible spatial correlation among visual entities.

The precision metrics pertaining to the trio of models are diligently documented in Table \ref{tab:accuracy}. For a more granular analytical insight, readers are directed to Tables \ref{tab:easy-acc} and \ref{tab:hard-acc}.
\begin{table}[h]
    \centering
    \caption{Accuracy of Three Models}
    \label{tab:accuracy}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \multicolumn{1}{c}{Model} & Easy      & Hard & Total \\ \midrule
    Bard                      &  $45.0\%$ & $42.5\%$ &  $43.75\%$ \\
    Visual ChatGPT            &  $62.5\%$ & $57.5\%$ &  $60.0\%$  \\
    MiniGPT-4                 &  $65.0\%$ & $57.5\%$ &  $61.25\%$  \\ \bottomrule
    \end{tabular}
\end{table}

Based on the metrics, the MiniGPT-4 model manifests superior accuracy, succeeded by Visual ChatGPT and subsequently Bard. Notably, Visual ChatGPT's performance closely parallels that of MiniGPT-4. Furthermore, the accuracy across the two question sets remains relatively consistent for each model. However, it's evident that the accuracy for the 'Easy' question set surpasses that of the 'Hard' set for every model. A more granular analysis will be presented in the ensuing section.

\section{Evaluation}
In this section, we will conduct an in-depth evaluation of the three models, deriving insights from the empirical results. The assessment will be segmented into three salient dimensions:Examination of Accuracy, Scrutiny of the Reasoning Process, and Deliberative Discussions.
\subsection{Examination of Accuracy}
The detailed wrong answer analysis is documented by Table \ref{tab:error-easy} and \ref{tab:error-hard} in Appendix \ref{Chart}. The analysis is predicated on the following categories:
\begin{itemize}
    \item \textbf{Left-Right Confusion:} The model confuses the left and right sides of two objects.
    \item \textbf{Recognition Error:} The model fails to recognize the object in the image or get the wrong object.
    \item \textbf{Judgement Fault in Front and Back:} The model fails to judge the front and back of the object or the spatial relationship between the front and back of the object.
    \item \textbf{Positional Fault:} The model gives the wrong spatial relationship between two objects and it is quite different from the image.
    \item \textbf{Item Misorientation:} The model gives the wrong spatial relationship because of getting a wrong orientation of the object.
    \item \textbf{Confused Expression:} The model gives a conflicting answer.
    \item \textbf{No Exact Answer:} The model gives an answer that is not sure or not exact.
    \item \textbf{No Response:} The model gives no response.
    \item \textbf{Error Judgement in Post:} The model give the wrong answer about the post of people or animals in the image.
    \item \textbf{Logic Fault:} The model gives the wrong answer because of the wrong logic.
    \item \textbf{Vertical Fault:} The model gives the wrong answer in the vertical direction.
\end{itemize}

Firstly, in Easy part, all models have problems in 'left-right confusion'and 'judgement fault in front and back'. The 'left-right confusion' is quite normal in this benchmark, the reason of this phenomena is hard to find, it appears with no significant points, and there are many error answers you can find in Appendix \ref{Easy}. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{../image set/easy/000000067428.jpg}
    \caption{Image of Question 14}
    \label{fig:left-right}
\end{figure}
The 'judgement fault in front and back' is another common error, the typical error answer is for Question 14, input image shown in Figure \ref{fig:left-right}, the question 14-2 is \textcolor{orange}{Is the horse on the back of the vehicle?} Answer: \textcolor{red}{Bard:} \textcolor{gray}{No, the horse is not on the back of the vehicle. The horse is in front of the vehicle, pulling it. The horse is attached to the vehicle by a harness, and the horse is walking forward, pulling the vehicle behind it.If the horse were on the back of the vehicle, it would be facing the opposite direction of the vehicle. The horse would also be attached to the vehicle by a different type of harness, such as a trailer hitch.The image you provided shows the horse in front of the vehicle, pulling it. This is a common sight in many parts of the world, where horses are used to pull vehicles such as carriages and carts.} \textcolor{red}{Visual ChatGPT:} \textcolor{gray}{The image shows a red truck parked next to a horse. The horse is not on the back of the vehicle.} \textcolor{red}{MiniGPT-4:} \textcolor{gray}{ No, the horse is not on the back of the vehicle. The horse is standing in front of the vehicle, next to it. The vehicle is a red pickup truck with a bed liner. The sky is clear and there are clouds in the background. The image is a scene of a horse and a pickup truck in a grassy field.} From this answer, it is clearly that all models have recognized horse and vehicle, 

\subsection{Scrutiny of the Reasoning Process}

\subsection{Deliberative Discussions}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\appendices
\input{Chart.tex}
\label{Chart}
\clearpage
\input{../Q&A/easy.tex}
\label{Easy}
\input{../Q&A/hard.tex}
\label{Hard}



\end{document}