\documentclass[journal,10pt]{IEEEtran}
\usepackage{sharina}

\begin{document}
\input{title.tex}
\section{Project Description}
In recent years, Large Language Models (LLMs) have transformed the field of Natural Language Processing (NLP) with their impressive achievements. These models, which have been trained on extensive amounts of text data, have proven to be highly effective in solving various NLP tasks, like translating languages, answering questions, and generating text. Their ability to understand and create text that resembles human language has been truly remarkable and easy to appreciate.

However, despite their proficiency in many linguistic tasks, LLMs have faced challenges when it comes to Spatial Reasoning. Unlike tasks that focus solely on language, Spatial Reasoning involves comprehending and manipulating visual and spatial information. To shed light on this limitation, I plan to conduct an experiment using popular LLMs like GPT-4\cite{peng2023instruction}. The main goal will be to test how well they can handle Visual Spatial Reasoning, which is important for understanding the relationships between objects in visual content.

The experiment will center around a commonly used task called Visual Question Answering (VQA). This task requires machines to answer questions based on provided images. For example, when given an image showing different objects, the machine will be given inquiries such as "What is the spatial relationship between object A and object B?". By designing carefully constructed tests like these, we can assess the level of Spatial Reasoning ability demonstrated by LLMs in an easy-to-understand manner.

Through evaluating their performance in VQA and their capacity to comprehend and reason about spatial relationships in visual content, we can gain valuable insights into the Spatial Reasoning capabilities of LLMs. These findings will not only enhance our understanding of the strengths and limitations of these models but also pave the way for further advancements in NLP. This progress will bring us closer to developing more comprehensive and versatile language models that are accessible and easy to work with.

\section{Aims and Objectives}
In this section, we will provide a clear outline of the aims and objectives of this project. The aims represent the overarching goals that we aim to achieve, while the objectives outline the specific steps and milestones that will be undertaken to fulfill these goals. The aims and objectives of this project are outlined below:
\subsection{Aims}
\begin{enumerate}
    \item To investigate the Spatial Reasoning abilities of Large Language Models (LLMs), specifically focusing on their performance in Visual Question Answering (VQA) tasks.
    \item To understand the limitations and challenges faced by LLMs in comprehending and reasoning about spatial relationships in visual content.
    \item To assess the current state of Spatial Reasoning capabilities in leading LLMs, then compare with elder models and explore their improvement in this domain. Besides, to identify potential avenues for enhancing LLMs' Spatial Reasoning abilities.
\end{enumerate}
\subsection{Objectives}
\begin{enumerate}
    \item Design and develop a comprehensive experimental framework for evaluating LLMs' Spatial Reasoning abilities in VQA tasks.
    \item Curate a diverse dataset of visual stimuli and corresponding questions that require spatial reasoning skills to answer accurately.
    \item Conduct systematic experiments using the prepared dataset and modified LLMs to assess their performance and measure their level of Spatial Reasoning competence.
    \item Analyze the experimental results to identify patterns, trends, and challenges in LLMs' Spatial Reasoning capabilities.
    \item Provide insights into the strengths and limitations of current LLMs for Spatial Reasoning tasks, based on the experimental findings.
    \item Suggest potential avenues for enhancing LLMs' Spatial Reasoning abilities, such as incorporating multimodal information or novel architectural modifications.
    \item Contribute to the existing body of knowledge in the field of NLP by advancing our understanding of LLMs' Spatial Reasoning abilities and their implications for future research and development.
\end{enumerate}

In summary, this project aims to investigate the Spatial Reasoning abilities of LLMs, specifically in VQA tasks, by designing experiments, analyzing results, and providing insights for improvement. By achieving these objectives, the project will contribute to the advancement of NLP and enhance our understanding of LLMs' capabilities in comprehending and reasoning about spatial relationships in visual content.

\section{Output}
During the experimental phase, an exhaustive assessment was undertaken on three leading LLM (Language and Logic Model) paradigms: Visual ChatGPT\cite{wu2023visual}, Bard, and MiniGPT-4\cite{zhu2023minigpt4}.

Visual ChatGPT represents a pioneering convergence of ChatGPT's linguistic capabilities with Visual Foundation Models. This synergy offers an enhanced interface with the AI, seamlessly integrating both textual and visual inputs. The model demonstrates proficiency in addressing intricate visual queries, executing visual editing tasks, and producing refined results. Its architectural framework is intricately designed to amalgamate visual model data with ChatGPT, making it adaptable to models with varied I/O channels and those necessitating visual feedback systems. Empirical research indicates that Visual ChatGPT offers a promising avenue for augmenting ChatGPT's visual capabilities in tandem with Visual Foundation Models\cite{wu2023visual}.

Bard, a prodigious creation from Google, emerges as a cutting-edge conversational AI, posing significant competition to OpenAI's ChatGPT. Recent innovations have equipped Bard with the capability to process visual data in tandem with textual information, marking a significant leap in multi-modal conversational AI. This advancement allows Bard to recognize and analyze visual components, such as images, when directed by textual prompts. While Bard's expertise in textual data processing is well-established, ongoing academic investigations are exploring its proficiency in visual understanding, highlighting its strengths and potential areas for refinement. Such academic endeavors are crucial for pinpointing limitations and steering the progression of multi-modal generative models\cite{qin2023good}.

MiniGPT-4, a trailblazing model, is engineered to encompass the expansive features of the GPT-4 model within a streamlined framework. It seamlessly fuses a static visual encoder with Vicuna, a robust language model, through a unique projection layer. This configuration enables MiniGPT-4 to exhibit a wide range of multi-modal generative capabilities inherent to GPT-4, ranging from crafting detailed image narratives to generating websites from handwritten schematics. Notably, MiniGPT-4 has revealed innovative competencies, such as formulating narratives influenced by visual cues and offering resolutions to visual challenges. The model's training methodology, focusing on precisely aligned and refined datasets, is instrumental in enhancing its generative accuracy\cite{zhu2023minigpt4}.

The metrics utilized for evaluation are detailed in Table \ref{tab:question_template}. The analysis comprises 80 question sets, bifurcated into two categories: Easy and Hard. This classification is predicated on the number of objects within the image and their respective scales. Images featuring no more than five objects, with a predominant object occupying a significant portion of the frame, are categorized as 'Easy'. Conversely, images displaying more than five objects, where each item has a comparatively modest presence, are labeled as 'Hard'. A comprehensive overview of the question sets, along with their corresponding outputs, is documented in Appendices \ref{Easy} and \ref{Hard}.
\begin{table}[ht]
    \centering
    \caption{Question Template}
    \label{tab:question_template}
    \begin{tabular}{|p{0.4\linewidth}|p{0.5\linewidth}|}
        \hline
        \textbf{Input Image} &\textbf{Questions}\\
        \hline
        \begin{center} \includegraphics[width=\linewidth]{../image set/hard/000000104739.jpg} \end{center}
        & \begin{enumerate}
            \item Can you describe the spatial relationship between these two laptops?
            \item Is this black laptop on the left of the white laptop?
            \item Show me your specific reasoning steps that lead you to the answer, better in detailed explanation.
        \end{enumerate}\\
        \hline
        \end{tabular}
\end{table}

The evaluation framework is structured around three pivotal dimensions:
\begin{enumerate}
\item Assessing the model's aptitude in discerning spatial relationships among object pairs.
\item Analyzing the model's competence in addressing queries emphasized by specific guiding terminologies.
\item Evaluating the model's skill in providing thorough rationales for its conclusions.
\end{enumerate}
The deliberate omission of accuracy metrics for the primary query is attributed to Bard's novel operational approach. In essence, Bard generates a broad array of potential conclusions, introducing challenges in conclusively affirming the veracity of a given response. This approach not only redefines the conventional notion of a 'correct' answer but also complicates the task of drawing equitable comparisons with analogous models.

As a result, the primary accuracy metric is derived from the responses to the second dimension, offering a more concrete benchmark for assessing truthfulness. The initial and final queries act as essential markers in gauging the model's depth of understanding and its capacity for inferential reasoning. The benchmark for accuracy is based on the alignment of the response with the spatial relationship delineated in the directive or the proposition of a credible spatial correlation among visual entities.

The precision metrics pertaining to the trio of models are diligently documented in Table \ref{tab:accuracy}. For a more granular analytical insight, readers are directed to Tables \ref{tab:easy-acc} and \ref{tab:hard-acc}.
\begin{table}[h]
    \centering
    \caption{Accuracy of Three Models}
    \label{tab:accuracy}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \multicolumn{1}{c}{Model} & Easy      & Hard & Total \\ \midrule
    Bard                      &  $45.0\%$ & $45.0\%$ &  $43.75\%$ \\
    Visual ChatGPT            &  $62.5\%$ & $60.0\%$ &  $57.5\%$  \\
    MiniGPT-4                 &  $65.0\%$ & $52.5\%$ &  $57.5\%$  \\ \bottomrule
    \end{tabular}
\end{table}

Upon a thorough examination of the accuracy metrics, it becomes evident that the performance of the three models necessitates further optimization. Within this ensemble, Bard consistently underperforms across various question sets. Visual ChatGPT, despite exhibiting vulnerabilities in the 'Easy' category, surpasses Bard and notably excels in the 'Hard' category. Conversely, MiniGPT-4 shines in the 'Easy' category but faces challenges in maintaining its superiority in the 'Hard' category. A detailed analysis of these findings will be elucidated in the subsequent section.

\section{Evaluation}
In this section, we will conduct an in-depth evaluation of the three models, deriving insights from the empirical results. The assessment will be segmented into three salient dimensions:Examination of Accuracy, Scrutiny of the Reasoning Process, and Deliberative Discussions.
\subsection{Examination of Accuracy}
\begin{table*}[]
    \centering
    \caption{Type of Error in Easy part of All Models}
    \label{tab:error-easy}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|}
    \hline
    Question Number & Bard                              & Visual ChatGPT                    & MiniGPT-4                         \\ \hline
    1               & left-right confusion              & -                                 & -                                 \\ \hline
    2               & left-right confusion              & -                                 & -                                 \\ \hline
    3               & -                                 & -                                 & left-right confusion              \\ \hline
    4               & -                                 & -                                 & left-right confusion              \\ \hline
    5               & recognition error                 & -                                 & -                                 \\ \hline
    6               & vertical judgement error          & vertical judgement error          & -                                 \\ \hline
    7               & -                                 & -                                 & left-right confusion              \\ \hline
    8               & -                                 & -                                 & -                                 \\ \hline
    9               & left-right confusion              & -                                 & left-right confusion              \\ \hline
    10              & -                                 & left-right confusion              & -                                 \\ \hline
    11              & error judgement in front and back & error judgement in front and back & error judgement in post           \\ \hline
    12              & error in direction of objects     & error in direction of objects     & -                                 \\ \hline
    13              & left-right confusion              & -                                 & -                                 \\ \hline
    14              & error judgement in front and back & error judgement in front and back & error judgement in front and back \\ \hline
    15              & error in position judgement       & error judgement in frame          & -                                 \\ \hline
    16              & recognition error                 & -                                 & -                                 \\ \hline
    17              & -                                 & -                                 & -                                 \\ \hline
    18              & -                                 & no response                       & -                                 \\ \hline
    19              & left-right confusion              & -                                 & left-right confusion              \\ \hline
    20              & -                                 & -                                 & -                                 \\ \hline
    21              & -                                 & left-right confusion              & -                                 \\ \hline
    22              & -                                 & -                                 & left-right confusion              \\ \hline
    23              & error judgement in front and back & error judgement in front and back & -                                 \\ \hline
    24              & left-right confusion              & -                                 & -                                 \\ \hline
    25              & -                                 & left-right confusion              & -                                 \\ \hline
    26              & -                                 & -                                 & -                                 \\ \hline
    27              & error judgement in front and back & error judgement in front and back & -                                 \\ \hline
    28              & error judgement in front and back & error judgement in front and back & error judgement in front and back \\ \hline
    29              & -                                 & -                                 & -                                 \\ \hline
    30              & vertical judgement error          & -                                 & -                                 \\ \hline
    31              & -                                 & -                                 & -                                 \\ \hline
    32              & -                                 & -                                 & error judegment in side           \\ \hline
    33              & error judgement in front and back & error judgement in front and back & error judgement in position       \\ \hline
    34              & error judgement in position       & -                                 & error judgement in position       \\ \hline
    35              & -                                 & -                                 & -                                 \\ \hline
    36              & -                                 & -                                 & -                                 \\ \hline
    37              & error judgement in position       & error judgement in position       & error judgement in position       \\ \hline
    38              & -                                 & -                                 & -                                 \\ \hline
    39              & error judgement in position       & error judgement in front and back & error judgement in front and back \\ \hline
    40              & error judgement in position       & -                                 & -                                 \\ \hline
    \end{tabular}%
    }
\end{table*}

\subsection{Scrutiny of the Reasoning Process}

\subsection{Deliberative Discussions}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\appendices
\input{Chart.tex}
\label{Chart}
\clearpage
\input{../Q&A/easy.tex}
\label{Easy}
\input{../Q&A/hard.tex}
\label{Hard}



\end{document}