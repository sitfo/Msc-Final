% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{peng2023instruction}
B.~Peng, C.~Li, P.~He, M.~Galley, and J.~Gao, ``Instruction tuning with gpt-4,'' \emph{arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{Hobbs1985-HOBFTO}
J.~R. Hobbs and R.~C. Moore, \emph{Formal Theories of the Commonsense World}.\hskip 1em plus 0.5em minus 0.4em\relax Intellect Books, 1985.

\bibitem{chen2015survey}
J.~Chen, A.~G. Cohn, D.~Liu, S.~Wang, J.~Ouyang, and Q.~Yu, ``A survey of qualitative spatial representations,'' \emph{The Knowledge Engineering Review}, vol.~30, no.~1, pp. 106--136, 2015.

\bibitem{cohn2008qualitative}
A.~G. Cohn and J.~Renz, ``Qualitative spatial representation and reasoning,'' \emph{Foundations of Artificial Intelligence}, vol.~3, pp. 551--596, 2008.

\bibitem{cohn2023dialectical}
A.~G. Cohn and J.~Hernandez-Orallo, ``Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of llms,'' \emph{arXiv preprint arXiv:2304.11164}, 2023.

\bibitem{lin2023using}
F.~Lin, Z.~Shou, and C.~Chen, ``Using language models for knowledge acquisition in natural language reasoning problems,'' \emph{arXiv preprint arXiv:2304.01771}, 2023.

\bibitem{borji2023categorical}
A.~Borji, ``A categorical archive of chatgpt failures,'' \emph{arXiv preprint arXiv:2302.03494}, 2023.

\bibitem{ullman2023large}
T.~Ullman, ``Large language models fail on trivial alterations to theory-of-mind tasks,'' \emph{arXiv preprint arXiv:2302.08399}, 2023.

\bibitem{li2019visualbert}
L.~H. Li, M.~Yatskar, D.~Yin, C.-J. Hsieh, and K.-W. Chang, ``Visualbert: A simple and performant baseline for vision and language,'' \emph{arXiv preprint arXiv:1908.03557}, 2019.

\bibitem{tan2019lxmert}
H.~Tan and M.~Bansal, ``Lxmert: Learning cross-modality encoder representations from transformers,'' \emph{arXiv preprint arXiv:1908.07490}, 2019.

\bibitem{chen2020uniter}
Y.-C. Chen, L.~Li, L.~Yu, A.~El~Kholy, F.~Ahmed, Z.~Gan, Y.~Cheng, and J.~Liu, ``Uniter: Universal image-text representation learning,'' in \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXX}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020, pp. 104--120.

\bibitem{7410636}
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~L. Zitnick, and D.~Parikh, ``Vqa: Visual question answering,'' in \emph{2015 IEEE International Conference on Computer Vision (ICCV)}, 2015, pp. 2425--2433.

\bibitem{liu2022visual}
F.~Liu, G.~Emerson, and N.~Collier, ``Visual spatial reasoning,'' \emph{arXiv preprint arXiv:2205.00363}, 2022.

\bibitem{hudson2019gqa}
D.~A. Hudson and C.~D. Manning, ``Gqa: A new dataset for real-world visual reasoning and compositional question answering,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2019, pp. 6700--6709.

\bibitem{grunde2021agqa}
M.~Grunde-McLaughlin, R.~Krishna, and M.~Agrawala, ``Agqa: A benchmark for compositional spatio-temporal reasoning,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 11\,287--11\,297.

\bibitem{liu2022things}
X.~Liu, D.~Yin, Y.~Feng, and D.~Zhao, ``Things not written in text: Exploring spatial commonsense from visual signals,'' \emph{arXiv preprint arXiv:2203.08075}, 2022.

\bibitem{zerroug2022benchmark}
A.~Zerroug, M.~Vaishnav, J.~Colin, S.~Musslick, and T.~Serre, ``A benchmark for compositional visual reasoning,'' \emph{arXiv preprint arXiv:2206.05379}, 2022.

\bibitem{parcalabescu2021valse}
L.~Parcalabescu, M.~Cafagna, L.~Muradjan, A.~Frank, I.~Calixto, and A.~Gatt, ``Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena,'' \emph{arXiv preprint arXiv:2112.07566}, 2021.

\bibitem{mirzaee2021spartqa}
R.~Mirzaee, H.~R. Faghihi, Q.~Ning, and P.~Kordjmashidi, ``Spartqa:: A textual question answering benchmark for spatial reasoning,'' \emph{arXiv preprint arXiv:2104.05832}, 2021.

\bibitem{burnell2023rethink}
R.~Burnell, W.~Schellaert, J.~Burden, T.~D. Ullman, F.~Martinez-Plumed, J.~B. Tenenbaum, D.~Rutar, L.~G. Cheke, J.~Sohl-Dickstein, M.~Mitchell \emph{et~al.}, ``Rethink reporting of evaluation results in ai,'' \emph{Science}, vol. 380, no. 6641, pp. 136--138, 2023.

\bibitem{davis2023benchmarks}
E.~Davis, ``Benchmarks for automated commonsense reasoning: A survey,'' \emph{arXiv preprint arXiv:2302.04752}, 2023.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan, P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in context,'' in \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2014, pp. 740--755.

\end{thebibliography}
