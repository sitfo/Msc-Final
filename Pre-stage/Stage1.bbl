% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{cohn2008qualitative}
A.~G. Cohn and J.~Renz, ``Qualitative spatial representation and reasoning,'' \emph{Foundations of Artificial Intelligence}, vol.~3, pp. 551--596, 2008.

\bibitem{cohn2023dialectical}
A.~G. Cohn and J.~Hernandez-Orallo, ``Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of llms,'' \emph{arXiv preprint arXiv:2304.11164}, 2023.

\bibitem{lin2023using}
F.~Lin, Z.~Shou, and C.~Chen, ``Using language models for knowledge acquisition in natural language reasoning problems,'' \emph{arXiv preprint arXiv:2304.01771}, 2023.

\bibitem{borji2023categorical}
A.~Borji, ``A categorical archive of chatgpt failures,'' \emph{arXiv preprint arXiv:2302.03494}, 2023.

\bibitem{7410636}
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~L. Zitnick, and D.~Parikh, ``Vqa: Visual question answering,'' in \emph{2015 IEEE International Conference on Computer Vision (ICCV)}, 2015, pp. 2425--2433.

\bibitem{liu2022visual}
F.~Liu, G.~Emerson, and N.~Collier, ``Visual spatial reasoning,'' \emph{arXiv preprint arXiv:2205.00363}, 2022.

\bibitem{hudson2019gqa}
D.~A. Hudson and C.~D. Manning, ``Gqa: A new dataset for real-world visual reasoning and compositional question answering,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2019, pp. 6700--6709.

\bibitem{grunde2021agqa}
M.~Grunde-McLaughlin, R.~Krishna, and M.~Agrawala, ``Agqa: A benchmark for compositional spatio-temporal reasoning,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 11\,287--11\,297.

\bibitem{zerroug2022benchmark}
A.~Zerroug, M.~Vaishnav, J.~Colin, S.~Musslick, and T.~Serre, ``A benchmark for compositional visual reasoning,'' \emph{arXiv preprint arXiv:2206.05379}, 2022.

\bibitem{parcalabescu2021valse}
L.~Parcalabescu, M.~Cafagna, L.~Muradjan, A.~Frank, I.~Calixto, and A.~Gatt, ``Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena,'' \emph{arXiv preprint arXiv:2112.07566}, 2021.

\bibitem{liu2022things}
X.~Liu, D.~Yin, Y.~Feng, and D.~Zhao, ``Things not written in text: Exploring spatial commonsense from visual signals,'' \emph{arXiv preprint arXiv:2203.08075}, 2022.

\bibitem{mirzaee2021spartqa}
R.~Mirzaee, H.~R. Faghihi, Q.~Ning, and P.~Kordjmashidi, ``Spartqa:: A textual question answering benchmark for spatial reasoning,'' \emph{arXiv preprint arXiv:2104.05832}, 2021.

\end{thebibliography}
